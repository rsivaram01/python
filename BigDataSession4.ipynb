{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0c26c4-f869-4b94-9c61-473cf956b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome\n"
     ]
    }
   ],
   "source": [
    "print(\"welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b716fb1-5e74-408e-8d91-90ee913820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 mb csv data ----> \n",
    "1000  100 mb  csv ------> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c6387f-1f25-4e4a-aa42-b28ec16aa2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*1000  -----> 1gb \n",
    "\n",
    "better compression\n",
    "able to run analytics on top of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703cefb-c222-4954-b3c8-6dbb668217f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a99760-a7db-468f-ac8e-e501e5af228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Serlization:-\n",
    "=================\n",
    "The process of converting data in memory to a format which can be sent over network or store on disk is called data Serlization\n",
    "\n",
    "com1 --> part1\n",
    "com2 --> part2 ----> \n",
    "\n",
    "\n",
    "Data deserlization:-\n",
    "==================\n",
    "it reverse ----> process of converting the data in the format which can be stored on memory is called data deserlization\n",
    "\n",
    "\n",
    "Serlization file formats:-\n",
    "=======================\n",
    "Avro \n",
    "Thrift\n",
    "Protocal buffers\n",
    "Sequence file \n",
    "\n",
    "\n",
    "Avro  --> most popular serlization fiel format \n",
    "\n",
    "what is avro?\n",
    "1. avro is one of the serlization format\n",
    "2. avro langunage indeipendant binary format of dara serlization\n",
    "3. avro stores schema along with data in the fromat of json, \n",
    "that's why when share this avro file with any other other applciation it can able read the data \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ddabe-a786-40a6-9270-aba38c78f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "File Fromats in Big Data world:-\n",
    "===============================\n",
    "\n",
    "csv, text, json, xml...etc\n",
    "\n",
    "csv, text   ------------->  there are row based file formats \n",
    "row based file format were suitable for CURD  ( create, update, read, delete)\n",
    "\n",
    "Column oriendted file formats:-\n",
    "------------------------------\n",
    "\n",
    "compression rate is very high  \n",
    "\n",
    "RCFILE   ---> record columnar File \n",
    "ORC      ---> Optimized row columner\n",
    "Parquet  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687c269-0a48-42cc-a7e0-74ee9e273027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORC:-\n",
    "====\n",
    "\n",
    "Optimized row columner\n",
    "\n",
    "1. it is adv version of RC file, which stores index along with data, which helps us to quickly look into the data as per the logic we have given.\n",
    "whener we store the data in the format of ORC --> it will create 250MB strips. ---->\n",
    "the compression format irc uses if snappy compression\n",
    "\n",
    "\n",
    "filename.snappy.orc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec625b-41d7-4b19-9bb5-58f1e64b8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parquet:-\n",
    "========\n",
    "\n",
    "Parquet is an another column format which is specifically deisgn for hadoop ecosystem. it can used wth any kind of \n",
    "data processing frame work, hadoop, mapreduce and apache spark\n",
    "\n",
    "whenever we store data in the format of parquet it stores that in 3 level of hierrachy structre\n",
    "row groups\n",
    "column chunks\n",
    "pages\n",
    "\n",
    "first it devided the data in row groups\n",
    "then it devides row groups into column chunks \n",
    "then columns again devided into pages\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879fcfc-3ddc-48d8-a2a3-7f5ad3690b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "where we use ORC files  \n",
    "Hive\n",
    "\n",
    "where we use parqut\n",
    "it could widely used in while writing the data \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
