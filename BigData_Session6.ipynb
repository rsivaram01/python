{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba5310-e6d2-4b05-a213-5a5aa52ee8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive\n",
    "\n",
    "1. how to create a hive table ( managed table / External table )\n",
    "2. what is partitioning and bucketing\n",
    "3. what is predicate pushdown\n",
    "4. how to load hive tables\n",
    "5. what is dead lock\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4373990-1f0d-4a4f-a43f-90dc3faadc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal table\n",
    "\n",
    "CREATE TABLE employee_internal (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    department STRING,\n",
    "    salary FLOAT\n",
    ")\n",
    "STORED AS ORC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dd6849-71cd-4d97-bdb2-3424c82c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# external table\n",
    "\n",
    "CREATE EXTERNAL TABLE employee_external (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    department STRING,\n",
    "    salary FLOAT\n",
    ")\n",
    "STORED AS ORC\n",
    "LOCATION '/data/cdf/emp_table_data/';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342336a-58a6-4db6-9de4-365093c96a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134aa6b-f5b8-48c6-8e08-b382cfecc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/data/cdf/emp_table_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5956b-2754-48b3-851e-443a030542d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data on hdfs from spurce ---------> by using spark raw to curated data --------> /data/cdf/emp_table_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399faca-4c70-4fdf-a334-140023e2e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('orc').save(\"emp.employee_internal\")\n",
    "\n",
    "\n",
    "df.write.format('orc').save(\"/data/cdf/emp_table_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba011389-7033-43f3-b849-f6b990513e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc emp.employee_external;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3292f3-e23c-4141-9b98-0ce31c4c4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW CREATE TABLE employee_external;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652376d-59cb-4c80-ac10-4ed4f70220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE `employee_external`(\n",
    "  `emp_id` int, \n",
    "  `name` string, \n",
    "  `department` string, \n",
    "  `salary` float)\n",
    "STORED AS ORC\n",
    "LOCATION\n",
    "  'hdfs://user/hadoop/external_data/employee'\n",
    "TBLPROPERTIES (\n",
    "  'EXTERNAL'='TRUE'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718b519-559c-41c3-ac1b-d3f3be9bb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " what is partitioning and bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996d9dd-bd44-4085-8882-8aec488bb4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71cbe-ca9d-432e-a5c6-32c0e4908059",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE employee_csv (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    department STRING,\n",
    "    salary FLOAT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/hadoop/external_data/employee_csv/';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4eda3-2a35-4302-b91e-0851bbc159a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d4ece-9439-42fd-93ca-9630cf05b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is partitioning and bucketing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e9f39-80d9-4c47-a129-3e7eaa128398",
   "metadata": {},
   "outputs": [],
   "source": [
    " # what is predicate pushdown\n",
    "\n",
    "reading emp table data by using spark  ---> 2L  ==> dataframe\n",
    "applying df1 = df.filter(col(\"age\")>35)  ====> 50k\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "predicate pushdown:-\n",
    "===================\n",
    "\n",
    "physical plan \n",
    "1st step\n",
    "\n",
    "push the filter to the database or datawarehouse ----> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0faa6-e505-4fb0-86fc-46fe469d5d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850c3ad-c912-4340-899e-dae399e6db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "apache sqoop \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a8b9f-a4df-4bce-ab64-9847b291ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# req:-?\n",
    "1. we want to do full load ---> extarct data from table1 place it in hdfs path\n",
    "\n",
    "dbname : cdf\n",
    "table name : email\n",
    "\n",
    "sqoop import -connect jbdc:mysql://hostname/cdf -username user1 -password passcode -table table1 -target-dir /hdfs/path/ -m 3\n",
    "\n",
    "# importing all tables by excluding some table\n",
    "\n",
    "sqoop import -connect jdbc:mysql://hostname/dbname -username user1 -password passcode  -target-dir /hdfs/path/ -exclude-table emp,adress -m 10 \n",
    "\n",
    "# File format of import?\n",
    "---------------------\n",
    "default importing type if CSV, but we can specify other delimeter as well\n",
    "\n",
    "tab delimeter \\t\n",
    "\n",
    "sqoop import -connect jdbc:mysql://hostname/dbname -username user1 -password passcode -query 'select * from emp where sal > 3' -target-dir /hdfs/path/ \n",
    "-filed-terminated-by '\\t' -enclosed-by '\"'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e400a-1b7a-4ee4-a24a-848f64756686",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distcp:-\n",
    "=======\n",
    "\n",
    "mappers:-\n",
    "-------\n",
    "\n",
    "parllesl process ----> -m 3\n",
    "3 parlell prcoess\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
