{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac7e28-35ad-48d0-bce1-df7442620562",
   "metadata": {},
   "outputs": [],
   "source": [
    "when you get call?\n",
    "\n",
    "skills set --> yes\n",
    "\n",
    "interview dates & time ---> interview slot\n",
    "\n",
    "at the interview\n",
    "\n",
    "1. introduce yourself\n",
    "2. explain me about your letest or current project arch\n",
    "\n",
    "\n",
    "day to day actvities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314c757-a61a-435e-876d-ae1a5909f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduce yourself:-\n",
    "\n",
    "our name\n",
    "our overall it exp\n",
    "relevent in DE\n",
    "\n",
    "I'm currelty workign with so and so.. as a data engineer'\n",
    "\n",
    "we should mention our primary skills\n",
    "python,sql,spark, hadoop, hdfs, hive, shell script, sqoop\n",
    "\n",
    "Azure stack:- databricks, azure synapse, adls\n",
    "\n",
    "secondary skills\n",
    "apache airflow, aws services :- ec2, s3, aws emr, aws glue, athena\n",
    "\n",
    "\n",
    "certification:-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618135bd-0f44-4808-9b6b-2f37b8ea8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain me about your letest or current project arch:-\n",
    "----------------------------------------------------\n",
    "\n",
    "2 projects\n",
    "\n",
    "latest project or current project ==========> Data hub Team\n",
    "last project  ===========> Data migration Team\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b937eb-2297-4aae-9d7a-3dd9afb1c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cliet name : statefarm  \n",
    "\n",
    "we are part of data hub team\n",
    "\n",
    "Data Hub ---> we received data from varois sources ---> we consolidate and this data we serve as per business need\n",
    "\n",
    "\n",
    "we should day \n",
    "-------------\n",
    "\n",
    "two env\n",
    "1. onprem ---> hadoop hdfs hive spark\n",
    "2. azure :- Databricks, Azure Synapse, Adls\n",
    "\n",
    "\n",
    "onprem:-\n",
    "------\n",
    "while coming to dataflow \n",
    "\n",
    "we will receive data from two sources --> informatica, mft server\n",
    "this data will land our HDFS - raw data zone\n",
    "then we consume the data and process data as per the business logic -- we will load data into hive external tables \n",
    "After laodong into hive tables ---> the data will get served for diffrent purpose , this data may consume by bi team and we will share reports with\n",
    "external vendors as well\n",
    "\n",
    "\n",
    "Azure:-\n",
    "------\n",
    "here will receive data from ADF --> Azure Data Factory, event hubs\n",
    "the data will land in ADLS\n",
    "After ADLS --> we will consume and process the data using databricks and load into unity catlouge ---> the data will get served for diffrent purpose , this data may consume by bi team and we will share reports with\n",
    "external vendors as well\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209e19c-cd9b-4113-acea-437382f024cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "which format you receive data?\n",
    "json, fixed widht file, csv\n",
    "\n",
    "whih format you are writing?\n",
    "on prem -- ORC\n",
    "Databricks -- Delta format ( open source file format - base file format is parquet )\n",
    "\n",
    "How much data you are handling daily?\n",
    "2 GB\n",
    "\n",
    "What is the size of your onprem cluster?\n",
    "20 node cluster , with total 2TB Meomory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c8a3a-6d73-46cc-8fc4-a4082856ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "day to day actvities:-\n",
    "---------------------\n",
    "\n",
    "Agile format --> we are having Project interation ---> 5 sprint\n",
    "\n",
    " we will be having PI planning calls ( requirement, work days estimation, dependencys ) --->  PI --> 90 days  ---> 5 sprint ( 14 days )\n",
    "\n",
    "\n",
    "Data analyst and business person or product owner ---> we develope pyspark as per the need and creating and loading tables as per req\n",
    "we create create workflow and schdule them\n",
    "\n",
    "if there is any issue in existing works flows I need to workon,\n",
    "\n",
    "if there is any perform issue --> I need to figure it out the issue had to fix the same\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
