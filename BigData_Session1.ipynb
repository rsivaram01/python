{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b8fe54-2b5d-4e60-9a71-2c2fce52a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d82c3b-4a36-47d4-bd7c-f0c087f8e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Big Data:-\n",
    "==========\n",
    "\n",
    "\n",
    "why we need big data tech?  :-\n",
    "\n",
    "what is big data?\n",
    "ans:- its huge volume of data and richness and the value we get out data\n",
    "\n",
    "3 char of big data?\n",
    "\n",
    "1. volume ---> amount of data that application genrates\n",
    "2. velocity  ---> this speed at which application generates data\n",
    "3. varacity  ---> integrity of data\n",
    "4. variety  ---> diffrent kinds of data ---> un structured, semi structured and structured\n",
    "\n",
    "un structured :- images, videos, audio\n",
    "semi structured : json, xml\n",
    "structured  - rdbms tables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549901a-6887-4176-8597-cb80a0f9b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82f979-78f1-4e29-902d-94f5035d528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hadoop:-\n",
    "------\n",
    "Its big data tech and higly scalabe and fault tolerrant\n",
    "\n",
    "it is tight couple of data_storage + computation_framework + resource_Manager\n",
    "\n",
    "\n",
    "cluster Manager\n",
    "Distributed Computer Engine\n",
    "Distributed file system\n",
    "\n",
    "\n",
    "Data Storage or Distributed file system :-\n",
    "========================================\n",
    "\n",
    "HDFS:-\n",
    "=====\n",
    "hadoop distributed file system\n",
    "\n",
    "fault tollerant\n",
    "\n",
    "\n",
    "100gb file  ---> in your hard disk  ---> I want to read 100gb file \n",
    "\n",
    "split 100gb file ---> 128 mb files ( default block size of hdfs )\n",
    "\n",
    "\n",
    "distributed file system:-\n",
    "=======================\n",
    "\n",
    "\n",
    "what is default strip size of hdfs?\n",
    "128mb \n",
    "\n",
    "where the all meta data will get stored?\n",
    "name node\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23a17e-ce56-4bb1-b173-4ba1e48d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Computer Engine :-\n",
    "===============================\n",
    "\n",
    "for processing the data we need some framework ---> Distributed Computer Engine\n",
    "\n",
    "Map reduce   ---> process data on disk, which used to do low level operations on data\n",
    "\n",
    "Apache Spark  --> process data in memory   --> it in memory computation engine\n",
    "\n",
    "speed diff :-\n",
    "at any time spark is 100x faster than mapreduce\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e8518-d2be-46a3-a3c1-23f540047a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster Manager :-\n",
    "resource manager --> which is used to manage our cluster\n",
    "\n",
    "yarn ---> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df358f-8fd0-4b91-ab5c-23cd30e4d211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
