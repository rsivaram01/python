{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1ec23-b5be-463a-9431-65581cd9b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "\n",
    "SQL\n",
    "\n",
    "Hadoop\n",
    "\n",
    "HDFS\n",
    "\n",
    "HIVE\n",
    "\n",
    "Apache Sqoop -----> current topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae981c9e-a2f0-4120-936e-a6508f4c6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainFrame ---->  DB2 ------> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17daadf4-ec09-47e2-9f67-8c033da29392",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract some data from DB2 ---->  he want to have some report on top of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45793d01-79ab-4214-8c32-1c3278a86801",
   "metadata": {},
   "outputs": [],
   "source": [
    "what a DE will do in this req?\n",
    "\n",
    "1. extract data from DB2 place it on HDFS\n",
    "2. By using you will process data as per the business logic  ( will be provided by either business user or BSA )\n",
    "3. After processing ---> you are going to load data in a table ( reporting_layer.emp_summary )\n",
    "\n",
    "we will be handover reporting_layer.emp_summary to power Bi Team ----> BI Team will create BI dashboard or report out of this data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f70bd-4033-431a-954c-f45f9e6349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. extract data from DB2 place it on HDFS     ===========> extarct data from DB2 to HDFS   ===> we need an import/ export to\n",
    "                                                                                                work on JDBC support databeses  (Apche SQoop)\n",
    "2. By using you will process data as per the business logic   ====> apache spark  ====> writing to the same path as we are giving in external table \n",
    "3. After processing ---> you are going to load data in a table ( reporting_layer.emp_summary )   ====> we will creating an hive external table \n",
    "                                                                                                    same path \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf0e4d-c691-468f-8469-9d1974bedf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44f989-e554-4f5d-99e6-345d376ff339",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. extract data from DB2 place it on HDFS :-\n",
    "============================================\n",
    "SQoop is an import and export tool, to copy data from jdbc connector supported databases \n",
    "\n",
    "DB2 to HDFS  ==> import\n",
    "HDFS to DB2  ==> export\n",
    "\n",
    "\n",
    "whenever we do table load we have two types of loads\n",
    "\n",
    "1. Full load or Onetime load\n",
    "2. incremenatl load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e22e4-18ed-4022-b0e3-7607ec2332cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full load or onetime load :-\n",
    "===========================\n",
    "\n",
    "you have one table on DB2 -- table1 ( contains 2000 records )\n",
    "\n",
    "req:- we want to have the same data which is acutaully present in DB2 ( table1 )   ===> HIVE\n",
    "\n",
    "today - 2000\n",
    "tmw -- 2100  ===> 100 record are incremental records\n",
    "\n",
    "\n",
    "hive:-\n",
    "----\n",
    "we use sqoop to import data whateer present in the table1  ===> on very 1st day  --> full load job\n",
    "now in hive we have 2000 records \n",
    "\n",
    "we need to incremnatal load\n",
    "\n",
    "2100 ---> append to our hive table ---> which is actually causes duplicates\n",
    "\n",
    "\n",
    "DB2 ---> to avoid duplicate issue we need to capture, new records =====> incremnatal load\n",
    "\n",
    "100\n",
    "\n",
    "2000 + 100 = 2100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6449e0f-e113-41c7-8b68-f33d70a34022",
   "metadata": {},
   "outputs": [],
   "source": [
    "           /data/raw/tabl1/ \n",
    "           /data/raw/tabl1/archive/2000 --->  100 record\n",
    "\n",
    "2am  ---> 30min\n",
    "\n",
    "3am ----> process /data/raw/tabl1/  ----> we will moved data into archive folder\n",
    "\n",
    "\n",
    "either job fail / success ===> Email --> notification sucess/ fail ---> SMTP \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bd011-5b9b-4a29-85f9-8ac4e0eb8db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
